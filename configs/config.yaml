# RevoltX Configuration File
# Edit this file to customize your job search automation

#-----------------------------------------------
# Core Search Settings
#-----------------------------------------------
search:
  keywords:
    - '"Data engineer" OR "data engineers"'
    - '"Big data specialists" OR "big data specialists"'
    - '"Senior data engineer" OR "Senior data engineers"'
    - '"Lead data engineer" OR "Lead data engineers"'
    - '"AI engineer" OR "AI engineers"'
    - '"AI developer" OR "AI developers"'
    - '"finops analyst" OR "finops analysts"'
    - '"cloud engineer" OR "cloud engineers"'
    - '"cloud platform engineer" OR "cloud platform engineers"'
    - '"platform engineer" OR "platform engineers"'
    - '"data consultant" OR "data consultants"'
    - '"data architect" OR "data architects"'
    - '"machine learning engineer" OR "machine learning engineers" OR "ML engineer" OR "ML engineers"'
  location: 'All-Australia' # Location to search in
  date_range: 2 # Number of days to look back, changed to 3 because it wasn't finding any new jobs
  sort_mode: 'ListedDate' # ListedDate, Relevance, etc.
  salary:
    min: 120000 # Minimum acceptable salary
    max: 250000 # Maximum salary to filter
    type: 'annual' # annual, hourly, daily, contract

#-----------------------------------------------
# Notifications
#-----------------------------------------------
notifications:
  slack:
    # Set this or use SLACK_WEBHOOK_URL environment variable
    webhook_url: '' # Add your Slack webhook URL here
    notify_on_error: true
    notify_on_warning: true
    notify_on_success: false
    pipelines:
      job_search:
        notify_on_error: true
        notify_on_warning: true
        notify_on_success: false
        channel: '#job-search-alerts' # Optional specific channel override
      job_outreach:
        notify_on_error: true
        notify_on_warning: true
        notify_on_success: false
        channel: '#job-outreach-alerts' # Optional specific channel override
      job_application:
        notify_on_error: true
        notify_on_warning: true
        notify_on_success: true
        channel: '#job-application-alerts' # Optional specific channel override

#-----------------------------------------------
# Your Profile
#-----------------------------------------------
name: 'William Marzella'
resume:
  preferences:
    aws_resume_id: 'b26a5dbc-0962-43fb-a093-facddea0c5ba'
    azure_resume_id: '493e381a-366f-4c2b-8b18-28791bd53252'
    mixed_resume_id: '5c3e6252-ae32-472a-a24a-305a4b697def'
  text:
    aws: |
      CAREER OBJECTIVES AND SUMMARY
      I am passionate about creating automated, high-integrity solutions for data problems in on-premises and cloud infrastructures with the right (not latest) software and applications. I use ETL/ELT tools with the purpose of developing sustainable systems that generate value and revenue. My most valuable attributes are (1) my ability to identify key performance indicators in data workflows that organisations need to inform critical decisions and (2) producing robust architecture and pipelines that make these performance data available for immediate action. I am extremely effective in maximising returns on investments of project deployment while meeting and exceeding contractual requirements within strict timelines. In a previous life, I was a sales engineer in automation software.

      COMPUTING SKILLS
      Infrastructure: Terraform, PostgreSQL, MS SQL Server, Oracle DB – native operating systems and Docker
      Integration: Databricks, Snowflake, Fivetran, Airbyte, dbt – organising and managing cloud databases
      Programming: Python, Bash, PowerShell, R, MATLAB – across Linux, Unix, and Windows
      Cloud: Amazon Web Services (AWS), Azure and Azure DevOps, GitHub, BitBucket, Spark (PySpark)
      Data Analytics: Tableau, Power BI, Qlik – analysing, interpreting, and presenting data

      HIGHLIGHTS OF CAPABILITIES
      Five (5) years expert skills in cloud database development and integration (Databricks, Snowflake) using ETL/ELT tools (dbt and Airbyte) and deploying cloud computing with AWS (EC2, RDS) and Azure (VM) cloud platforms

      Four (4) years' experience establishing practices for developing CI/CD pipelines to ensure high-fidelity code through version control (Git) and automation (Git Actions, Azure Pipelines) and to maximise collaboration within teams

      Eight (8) years of specialised SQL database development, computing skills, and programming (Python, MATLAB), as well as gathering/collecting, cleaning/cleansing, and analysing discrete and time series data with statistics (R, SPSS, jamovi)

      EDUCATION
      University of Southern California	Los Angeles, CA
      BS, Mechanical Engineering	Graduation Date: 2020
      Built robots combining mechanical design and control software. Experience with CNC machining, 3D printing, and CAD/CAM tools. Design and simulate mechanical systems before manufacturing.

      PROFESSIONAL CERTIFICATIONS
      2025 - Azure Databricks & Spark For Data Engineers (PySpark / SQL)
      2023 - AWS Solutions Architect
      2023 - Cloud Computing with Amazon Web Services

      EXPERIENCE (FULL TIME)
      Alfab Pty Ltd, Melbourne, VIC 	October 2023 - Present
      Australia's leading manufacturer of marine and automotive glass products and aluminum fabrications.

      Senior Data Engineer
      I plan, design, and deliver high quality solutions to maximize operational efficiency and promote manufacturing excellence within a high-volume aluminum production facility while working with highly sensitive, proprietary manufacturing data. I conduct end-to-end business intelligence analysis with internal and external stakeholders – such as our Manufacturing Operations team, raw materials suppliers, and software vendors – to ensure the data resources I architect enhance our production team's effectiveness.

      Designed, developed, and deployed data pipelines for migrating manufacturing production data from separate SQL Server (on-premise) and Oracle databases to Snowflake platform (hosted on AWS), leveraging Matillion for Extraction/Load and dbt for Transformation, with Change Data Capture (CDC or 'delta') from the source data to our cloud deployment
      In collaboration with production managers, process engineers, and quality assurance teams, refactored SQL Server data warehouse and developed an automated pipeline (stored procedures, SSIS, SQL Server Agent) from an existing manual process for delivering monthly reporting to executive stakeholders, which produced new methods for delivering production efficiency metrics using routinely collected data and, in the process, generated an additional $500k cost savings per month for the manufacturing facility.
      Developed and deployed high-fidelity pipeline tests, following SRP (single responsibility principle), to ensure data integrity and accuracy is maintained throughout the lifespan of the pipelines I was responsible for
      Introduced and developed standard operating procedures for version control (Git), pull requests (Azure DevOps), SQL linting (SLQFluff), and code review and implemented automated CI/CD (Azure Pipelines), building capacity in 5 colleagues in the use of Git, Azure DevOps, and GitOps best practices	

      Tray.io, San Diego, CA	April 2021 - October 2023
      Enterprise integration platform enabling automated workflows across cloud and on-premise systems

      Platform Engineer	
      This position existed in a dynamic team of software engineers and data experts delivering innovative solutions for enterprise clients across healthcare and fintech sectors. I led the design, development, and execution of end-to-end solutions that enabled improved business processes and data integrity through advanced analytics and machine learning applications.

      Led the design, development, and deployment of an end-to-end ELT pipeline to enable high-fidelity, advanced data analytics on the Azure cloud platform, leveraging Databricks, Azure Data Factory, and Azure Data Lake
      Coordinated and mentored cross-functional teams of data engineers, data scientists, and data analysts to deliver robust business solutions that met critical requirements for enterprise healthcare and fintech clients
      Developed supervised and unsupervised machine learning models, leveraging Keras, PyTorch, and Tensorflow, to create Retrieval Augmented Generation (RAG) applications for intelligent document processing and knowledge discovery
      Organized data collection of very large time series datasets (> 2000 features) as input to supervised and unsupervised machine learning models I developed
      Conducted longitudinal analysis and statistical testing to evaluate customer behavior patterns by uncovering and interpreting trends in discrete and time series data

      Chilton's Artisan Foods, Melbourne, VIC	July 2019 - April 2021
      Food manufacturer making premium wholesale bakery products in Melbourne using local Australian ingredients.

      Data Engineer
      I developed data pipelines for ingesting, transforming, and providing access to data in relational databases through on-premises and cloud platforms. I created data warehouse solutions to make data readily available to data analysts in the form of data marts. I also developed visualisations in Power BI for translating key findings to executives.

      Responsible for assisting in the development of the manufacturing facility's Databricks deployment
      Assisted in the successful migration of all on-premise legacy packages (SSIS) into Azure Data Factory
      Migrated and automated RBAC processes from fragile legacy protocols into an auditable standalone application, complete with unit-testing, which eliminated the possibility of a large class of security breaches
      Wrote Python code to faultlessly parse hundreds of Microsoft Access databases, totalling millions of records, into a relational structure for ingesting to SQL Server
      Engineering & Academic Portfolio, Remote	December 2015 - June 2019
      Combined academic and professional engineering practice focusing on robotics, automation, and systems integration

      Higher Education, Management, and Leadership
      I studied to complete my mechanical engineering degree from February 2016 to December 2020. Throughout my education and early career, I gained hands-on engineering and leadership experience:

      Led teams in designing and manufacturing custom robotics solutions, utilising 3D CAD software and modern manufacturing techniques including additive manufacturing
      Managed client relationships and negotiated manufacturing contracts while working closely with stakeholders to deliver precision-engineered solutions
      Developed automation solutions combining mechanical systems with programming, which sparked my interest in data engineering and software development
      Coordinated cross-functional engineering teams in prototyping and iterating designs based on customer feedback and manufacturing constraints
      Leveraged programming skills learned through robotics and automation to transition into data engineering, applying similar principles of systematic problem-solving and optimisation

      EXPERIENCE (CONTRACTING)

      Motis Group, Melbourne, VIC	June 2022 - Present
      Technology consultancy specializing in enterprise automation and system integration solutions

      Principal Cloud Engineer
      I design solutions to data and database problems in the public and private sectors, specializing in enterprise-scale data architecture and infrastructure. I develop relational databases and data pipelines using ETL/ELT tools for both local and cloud infrastructure, adhering to best practice and strict data governance.

      Deployed a Dockerised ETL application (Airbyte) in Linux into a cloud environment (EC2 in AWS) to migrate data from an on-premises (PostgreSQL) to cloud infrastructure (Snowflake) for a commercial retail application with over a terabyte of data
      Developed automated cloud pipelines (Azure Data Factory) for ingesting data (Databricks, PySpark) from a variety of sources (SQL, JSON, parquet, CSV) into data lakes (Azure Data Lake) to facilitate analysis of open-source Formula1 data
      Deployed an AWS RDS instance to migrate data from an existing locally hosted infrastructure (Snowflake hosted in AWS) using ELT pipelines developed with Matillion and dbt for a private medical company

      EXTRA CURRICULAR & VOLUNTEER ACTIVITIES
      USC TRANSFER STUDENT COMMUNITY, Los Angeles, CA 2020-2021
      Board Member – Weekly newsletter columnist with an email list of ~800 students. Produced a podcast interviewing 50+ transfer students and alumni.

      SURREY PARK SWIMMING CLUB, Melbourne, VIC 2011-2018
      National Level Swimmer – Competed at a National level for 50m/100m Freestyle and Butterfly for the State of Victoria

    azure: |
      CAREER OBJECTIVES AND SUMMARY
      I am passionate about creating automated, high-integrity solutions for data problems in on-premises and cloud infrastructures with the right (not latest) software and applications. I use ETL/ELT tools with the purpose of developing sustainable systems that generate value and revenue. My most valuable attributes are (1) my ability to identify key performance indicators in data workflows that organisations need to inform critical decisions and (2) producing robust architecture and pipelines that make these performance data available for immediate action. I am extremely effective in maximising returns on investments of project deployment while meeting and exceeding contractual requirements within strict timelines. In a previous life, I was a sales engineer in automation software.

      COMPUTING SKILLS
      Infrastructure: Terraform, PostgreSQL, MS SQL Server, Oracle DB – native operating systems and Docker
      Integration: Databricks, Snowflake, Fivetran, Airbyte, dbt – organising and managing cloud databases
      Programming: Python, Bash, PowerShell, R, MATLAB – across Linux, Unix, and Windows
      Cloud: Amazon Web Services (AWS), Azure and Azure DevOps, GitHub, BitBucket, Spark (PySpark)
      Data Analytics: Tableau, Power BI, Qlik – analysing, interpreting, and presenting data

      HIGHLIGHTS OF CAPABILITIES
      Five (5) years expert skills in cloud database development and integration (Azure Data Factory) using ETL/ELT tools (Databricks, PySpark) and deploying cloud computing on the Azure cloud platform leveraging Azure VM

      Four (4) years' experience establishing practices for developing CI/CD pipelines to ensure high-fidelity code through version control (Git) and automation (Azure Pipelines, Git Actions) and to maximise collaboration within teams

      Eight (8) years of specialised SQL database development, computing skills, and programming (Python, MATLAB), as well as gathering/collecting, cleaning/cleansing, and analysing discrete and time series data with statistics (R, SPSS, jamovi)

      EDUCATION
      University of Southern California	Los Angeles, CA
      BS, Mechanical Engineering	Graduation Date: 2020
      Built robots combining mechanical design and control software. Experience with CNC machining, 3D printing, and CAD/CAM tools. Design and simulate mechanical systems before manufacturing.

      PROFESSIONAL CERTIFICATIONS
      2025 - Azure Databricks & Spark For Data Engineers (PySpark / SQL)
      2023 - AWS Solutions Architect
      2023 - Cloud Computing with Amazon Web Services

      EXPERIENCE (FULL TIME)
      Alfab Pty Ltd, Melbourne, VIC 	October 2023 - Present
      Australia's leading manufacturer of marine and automotive glass products and aluminum fabrications.

      Senior Data Engineer
      I plan, design, and deliver high quality solutions to maximize operational efficiency and promote manufacturing excellence within a high-volume aluminum production facility while working with highly sensitive, proprietary manufacturing data. I conduct end-to-end business intelligence analysis with internal and external stakeholders – such as our Manufacturing Operations team, raw materials suppliers, and software vendors – to ensure the data resources I architect enhance our production team's effectiveness.

      Designed, developed, and deployed data pipelines for migrating manufacturing production data from separate SQL Server (on-premise) and Oracle databases to the Azure cloud platform, leveraging Azure Data Factory to orchestrate data extraction and load (using Databricks) into ADL and transformation (using PySpark) into Azure Synapse Analytics, with Change Data Capture (CDC or 'delta') from the source data to our cloud deployment
      In collaboration with production managers, process engineers, and quality assurance teams, refactored SQL Server data warehouse and developed an automated pipeline (stored procedures, SSIS, SQL Server Agent) from an existing manual process for delivering monthly reporting to executive stakeholders, which produced new methods for delivering production efficiency metrics using routinely collected data and, in the process, generated an additional $500k cost savings per month for the manufacturing facility.
      Developed and deployed high-fidelity pipeline tests, following SRP (single responsibility principle), to ensure data integrity and accuracy is maintained throughout the lifespan of the pipelines I was responsible for
      Introduced and developed standard operating procedures for version control (Git), pull requests (Azure DevOps), SQL linting (SLQFluff), and code review and implemented automated CI/CD (Azure Pipelines), building capacity in 5 colleagues in the use of Git, Azure DevOps, and GitOps best practices	

      Tray.io, San Diego, CA	April 2021 - October 2023
      Enterprise integration platform enabling automated workflows across cloud and on-premise systems

      Platform Engineer	
      This position existed in a dynamic team of software engineers and data experts delivering innovative solutions for enterprise clients across healthcare and fintech sectors. I led the design, development, and execution of end-to-end solutions that enabled improved business processes and data integrity through advanced analytics and machine learning applications.

      Led the design, development, and deployment of an end-to-end ELT pipeline to enable high-fidelity, advanced data analytics on the Azure cloud platform, leveraging Databricks, Azure Data Factory, and Azure Data Lake
      Coordinated and mentored cross-functional teams of data engineers, data scientists, and data analysts to deliver robust business solutions that met critical requirements for enterprise healthcare and fintech clients
      Developed supervised and unsupervised machine learning models, leveraging Keras, PyTorch, and Tensorflow, to create Retrieval Augmented Generation (RAG) applications for intelligent document processing and knowledge discovery
      Organized data collection of very large time series datasets (> 2000 features) as input to supervised and unsupervised machine learning models I developed
      Conducted longitudinal analysis and statistical testing to evaluate customer behavior patterns by uncovering and interpreting trends in discrete and time series data

      Chilton's Artisan Foods, Melbourne, VIC	July 2019 - April 2021
      Food manufacturer making premium wholesale bakery products in Melbourne using local Australian ingredients.

      Data Engineer
      I developed data pipelines for ingesting, transforming, and providing access to data in relational databases through on-premises and cloud platforms. I created data warehouse solutions to make data readily available to data analysts in the form of data marts. I also developed visualisations in Power BI for translating key findings to executives.

      Responsible for assisting in the development of the manufacturing facility's Databricks deployment
      Assisted in the successful migration of all on-premise legacy packages (SSIS) into Azure Data Factory
      Migrated and automated RBAC processes from fragile legacy protocols into an auditable standalone application, complete with unit-testing, which eliminated the possibility of a large class of security breaches
      Wrote Python code to faultlessly parse hundreds of Microsoft Access databases, totalling millions of records, into a relational structure for ingesting to SQL Server

      Engineering & Academic Portfolio, Remote	December 2015 - June 2019
      Combined academic and professional engineering practice focusing on robotics, automation, and systems integration

      Higher Education, Management, and Leadership
      I studied to complete my mechanical engineering degree from February 2016 to December 2020. Throughout my education and early career, I gained hands-on engineering and leadership experience:

      Led teams in designing and manufacturing custom robotics solutions, utilising 3D CAD software and modern manufacturing techniques including additive manufacturing
      Managed client relationships and negotiated manufacturing contracts while working closely with stakeholders to deliver precision-engineered solutions
      Developed automation solutions combining mechanical systems with programming, which sparked my interest in data engineering and software development
      Coordinated cross-functional engineering teams in prototyping and iterating designs based on customer feedback and manufacturing constraints
      Leveraged programming skills learned through robotics and automation to transition into data engineering, applying similar principles of systematic problem-solving and optimisation

      EXPERIENCE (CONTRACTING)

      Motis Group, Melbourne, VIC	June 2022 - Present
      Technology consultancy specializing in enterprise automation and system integration solutions

      Principal Cloud Engineer
      I design solutions to data and database problems in the public and private sectors, specializing in enterprise-scale data architecture and infrastructure. I develop relational databases and data pipelines using ETL/ELT tools for both local and cloud infrastructure, adhering to best practice and strict data governance.

      Developed automated cloud pipelines (Azure Data Factory) for ingesting data (Databricks, PySpark) from a variety of sources (SQL, JSON, parquet, CSV) into data lakes (Azure Data Lake) to facilitate analysis of highly sensitive data
      Deployed a Dockerised ETL application (Airbyte) in Linux into a cloud environment (EC2 in AWS) to migrate data from an on-premises (PostgreSQL) to cloud infrastructure (Snowflake) for a commercial retail application with over a terabyte of data
      Deployed an AWS RDS instance to migrate data from an existing locally hosted infrastructure (Snowflake hosted in AWS) using ELT pipelines developed with Matillion and dbt for a private medical company

      EXTRA CURRICULAR & VOLUNTEER ACTIVITIES
      USC TRANSFER STUDENT COMMUNITY, Los Angeles, CA 2020-2021
      Board Member – Weekly newsletter columnist with an email list of ~800 students. Produced a podcast interviewing 50+ transfer students and alumni.

      SURREY PARK SWIMMING CLUB, Melbourne, VIC 2011-2018
      National Level Swimmer – Competed at a National level for 50m/100m Freestyle and Butterfly for the State of Victoria

#-----------------------------------------------
# Technical Settings
#-----------------------------------------------
platforms:
  seek:
    enabled: true
  indeed:
    enabled: false

scraping:
  max_jobs: 0 # Max jobs per run (0 = unlimited)
  delay_seconds: 1 # Delay between requests
  timeout_seconds: 10 # Request timeout
  quick_apply_only: true # Only apply to jobs with quick apply enabled

analysis:
  min_score: 0 # Minimum match score (0-100)
  model: gpt-4o # OpenAI model to use
